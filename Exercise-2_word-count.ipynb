{"cells":[{"cell_type":"markdown","source":["# HackOnData.com\n### Instructions\nPerform the following operations to the text:\n\n   - Remove punctuation, convert text to lower case, and strip leading and trailing spaces. Then:\n     - Find the top 10 most popular words\n     - Find the top 10 most popular characters\n   - In addition to removing punctuation (see previous question) remove words that are included in the list -- http://tacit.usc.edu/resources/stopwords_eng.txt --. Then:\n     - Find the top 10 most popular words after removing the words above\n     - Find the words and counts for string starting with \"dra\". Does it give you an idea of other sorts of text preprocessing that you can perform?\n   - Find the most common words used before the word \"dragon\". Example: in the phrase \"the red dragon\", \"red\" is the word before \"dragon\". Make sure to exclude the list of words of the previous question."],"metadata":{}},{"cell_type":"code","source":["CONNECTION = (\n  '*',                                         # AWS_KEY\n  '*',                                         # AWS_PWD\n  '*',                                         # BUCKET\n)\nRESOURCES = (\n  'Dragons+and+Dragon+Lore.txt',               # RAW_CONTENT_FILENAME\n  'stopwords_eng.txt',                         # STOPWORDS\n)\n\nPATTERN = 'dra'\nREGEX = '[\\w]+(?=\\sdragon)'\n\nWATERFALL_LIMIT = 10\nraw_text, raw_stopwords = (sc.textFile('s3n://%s:%s@%s/%s' % (CONNECTION + (r,)), minPartitions=4) \n                                       for r in RESOURCES)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["import operator\nimport re\nimport string\n\n# Declarations\npunctuation = string.punctuation # !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \n\ndef _char_count(tpl):\n  \"\"\"\n  Input: <tuple of a (word, it's frequency in text)\n  Algo:\n    Count chars in a word, \n    then multiply char count by frequency of the word\n  \"\"\"\n  d = {}\n  word, frequency = tpl\n  for c in word:\n    d[c] = d.get(c, 0) + 1\n  return map(\n    lambda k_v: (k_v[0], k_v[1] * frequency),\n    d.items())\n\ndef _stats(data, *args):\n  \"\"\"\n  Not reusable. \n  Requires both args and data elements to be length of 3 \n  (string formatting restriction)\n  \"\"\"\n  return '\\n'.join((\n    '|\\t%s\\t|\\t%s\\t|\\t%s\\t|' % args, \n    '|%s|' % ('-'*55),\n    '\\n'.join(map(\n        lambda tpl: '|\\t%s\\t\\t%s\\t\\t%s\\t\\t|' % (tpl[0]+1, tpl[1][0],tpl[1][1]), \n        enumerate(data)))))\n\ndef pattern_search(line):                                    \n  matches = re.findall(REGEX, line, flags=re.IGNORECASE)\n  if matches:\n      return tuple((m, 1) for m in matches)\n  return ()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["orig_text = (raw_text\n        .map(lambda l: ''.join(c for c in l if c not in punctuation))  # drop punctuation\n        .map(lambda l: l.strip() or None)                              # remove whitespaces\n        .filter(None)                                                  # remove empty lines        \n        .map(lambda l: l.lower()))                                     # lowercase\n\nsplit_by_words = orig_text.flatMap(lambda l: l.split())                # split by whitespace -- RDD will be reused\n\n_reducer = lambda rdd: (rdd                                            # -- TRANSFORMATION will be reused\n  .map(lambda w: (w, 1))                                               # create tuples\n  .reduceByKey(operator.add))                                          # count frequencies\n\nword_tuples = _reducer(split_by_words)\n\nwords_by_frequency = word_tuples.sortBy(lambda tpl: tpl[1], False)\ncharacters_by_frequency = (words_by_frequency\n  .flatMap(_char_count)\n  .reduceByKey(operator.add)\n  .sortBy(lambda tpl: tpl[1], False))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["\nprint '| Most frequent words %s' % ('-'*35)\nprint _stats(                                                         # 1. most frequent words\n  words_by_frequency.take(WATERFALL_LIMIT), \n  'POS', 'WORD', 'FREQUENCY')\n\n\nprint '\\n| Most frequent characters: %s' % ('-'*29)\nprint _stats(                                                         # 2. most frequent characters\n  characters_by_frequency.take(WATERFALL_LIMIT), \n  'POS', 'CHAR', 'FREQUENCY')"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["cleaned_subset = split_by_words.subtract(raw_stopwords)\ncleaned_subset_by_frequency = _reducer(cleaned_subset).sortBy(lambda tpl: tpl[1], False)\n\nprint '| Most frequent words * NO STOPWORDS * %s' % ('-'*18)\nprint _stats(                                                         # 3. most frequent words without stopwords\n  cleaned_subset_by_frequency.take(WATERFALL_LIMIT), \n  'POS', 'WORD', 'FREQUENCY')"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["dras = cleaned_subset_by_frequency.filter(lambda tpl: tpl[0].startswith(PATTERN))\ndisplay(dras.toDF(schema=['Word', 'Frequency']))                      # 4. words starting with PATTERN (by frequency)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["stops = set(raw_stopwords.collect())\npunctuation_minus_stop = ''.join(set(string.punctuation) - set(['.',]))\nc_text = (raw_text\n        .map(lambda l: ''.join(c for c in l if c not in punctuation_minus_stop))  # drop all punctuation except fulll stops\n        .map(lambda l: l.lower())                                      \n        .flatMap(lambda l: l.split('.'))\n        .map(lambda l: l.strip() or None)                              # remove whitespaces\n        .filter(None)                                                  # remove empty lines        \n        .map(lambda l: ' '.join(w for w in l.split() if w not in stops)))\n\nword_before_dragons = (c_text                                         #5 Find mot frequent words before \"dragon\"\n    .flatMap(pattern_search)\n    .filter(None)\n    .reduceByKey(operator.add)\n    .sortBy(lambda tpl: tpl[1], False))\n\ndisplay(word_before_dragons.toDF(schema=['Word', 'Frequency']))"],"metadata":{},"outputs":[],"execution_count":8}],"metadata":{"name":"Exercise-2_word-count","notebookId":691085516512473},"nbformat":4,"nbformat_minor":0}
