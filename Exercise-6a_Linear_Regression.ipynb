{"cells":[{"cell_type":"markdown","source":["# HackOnData.com\n\n## Exercise #6 - Linear Regression"],"metadata":{}},{"cell_type":"markdown","source":["### Week 6 Lab 1 (Public link to the lab solution):\nhttps://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/6762635509390866/1784028329753015/7348375132556489/latest.html"],"metadata":{}},{"cell_type":"code","source":["%%bash\n\nS3PATH=\"https://s3-us-west-2.amazonaws.com/hackondata.evaluation\"\nFILES=(\"OnlineNewsTesting.csv\" \"OnlineNewsTrainingAndValidation.csv\")\n\nfor file in \"${FILES[@]}\"\ndo\n    wget $S3PATH/$file\ndone"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%python\n\nproj_folder = \"/mnt/E6\"\nfilenames = \"OnlineNewsTesting.csv\", \"OnlineNewsTrainingAndValidation.csv\"\n\ndbutils.fs.mkdirs(proj_folder)\n[dbutils.fs.cp(\"file:/databricks/driver/%s\" % f, proj_folder) for f in filenames]"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["import os\nfrom pyspark.sql import Row\n\nl = lambda f: (sqlContext\n    .read.format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\")\n    .option(\"inferschema\", \"true\")\n    .option(\"mode\", \"DROPMALFORMED\")\n    .load(os.path.join(proj_folder, f))\n    .map(lambda r: Row(**{k.strip(): v \n                          for k, v in r.asDict().iteritems()})))\n\nraw_travRDD, raw_tstRDD = map(l, filenames)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["display(sqlContext.createDataFrame(raw_travRDD))"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["url_sharesRDD = raw_travRDD.map(lambda r: Row(url=r.url, shares=r.shares))\ndisplay(sqlContext.createDataFrame(url_sharesRDD).orderBy('shares', ascending=False))"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["0. url: URL of the article (non-predictive) \n1. timedelta: Days between the article publication and the dataset acquisition (non-predictive) \n2. n_tokens_title: Number of words in the title \n3. n_tokens_content: Number of words in the content \n4. n_unique_tokens: Rate of unique words in the content \n5. n_non_stop_words: Rate of non-stop words in the content \n6. n_non_stop_unique_tokens: Rate of unique non-stop words in the content \n7. num_hrefs: Number of links \n8. num_self_hrefs: Number of links to other articles published by Mashable \n9. num_imgs: Number of images \n10. num_videos: Number of videos \n11. average_token_length: Average length of the words in the content \n12. num_keywords: Number of keywords in the metadata \n13. data_channel_is_lifestyle: Is data channel 'Lifestyle'? \n14. data_channel_is_entertainment: Is data channel 'Entertainment'? \n15. data_channel_is_bus: Is data channel 'Business'? \n16. data_channel_is_socmed: Is data channel 'Social Media'? \n17. data_channel_is_tech: Is data channel 'Tech'? \n18. data_channel_is_world: Is data channel 'World'? \n19. kw_min_min: Worst keyword (min. shares) \n20. kw_max_min: Worst keyword (max. shares) \n21. kw_avg_min: Worst keyword (avg. shares) \n22. kw_min_max: Best keyword (min. shares) \n23. kw_max_max: Best keyword (max. shares) \n24. kw_avg_max: Best keyword (avg. shares) \n25. kw_min_avg: Avg. keyword (min. shares) \n26. kw_max_avg: Avg. keyword (max. shares) \n27. kw_avg_avg: Avg. keyword (avg. shares) \n28. self_reference_min_shares: Min. shares of referenced articles in Mashable \n29. self_reference_max_shares: Max. shares of referenced articles in Mashable \n30. self_reference_avg_sharess: Avg. shares of referenced articles in Mashable \n31. weekday_is_monday: Was the article published on a Monday? \n32. weekday_is_tuesday: Was the article published on a Tuesday? \n33. weekday_is_wednesday: Was the article published on a Wednesday? \n34. weekday_is_thursday: Was the article published on a Thursday? \n35. weekday_is_friday: Was the article published on a Friday? \n36. weekday_is_saturday: Was the article published on a Saturday? \n37. weekday_is_sunday: Was the article published on a Sunday? \n38. is_weekend: Was the article published on the weekend? \n39. LDA_00: Closeness to LDA topic 0 \n40. LDA_01: Closeness to LDA topic 1 \n41. LDA_02: Closeness to LDA topic 2 \n42. LDA_03: Closeness to LDA topic 3 \n43. LDA_04: Closeness to LDA topic 4 \n44. global_subjectivity: Text subjectivity \n45. global_sentiment_polarity: Text sentiment polarity \n46. global_rate_positive_words: Rate of positive words in the content \n47. global_rate_negative_words: Rate of negative words in the content \n48. rate_positive_words: Rate of positive words among non-neutral tokens \n49. rate_negative_words: Rate of negative words among non-neutral tokens \n50. avg_positive_polarity: Avg. polarity of positive words \n51. min_positive_polarity: Min. polarity of positive words \n52. max_positive_polarity: Max. polarity of positive words \n53. avg_negative_polarity: Avg. polarity of negative words \n54. min_negative_polarity: Min. polarity of negative words \n55. max_negative_polarity: Max. polarity of negative words \n56. title_subjectivity: Title subjectivity \n57. title_sentiment_polarity: Title polarity \n58. abs_title_subjectivity: Absolute subjectivity level \n59. abs_title_sentiment_polarity: Absolute polarity level \n60. shares: Number of shares (target)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["## Linear Regression:\n\nThis week we are working with linear regression:\n\n- We have split the dataset in two. Please use the first part for training, testing, and validating your model. Once you have your best model evaluate it using the second dataset for scoring only. The purpose of the scoring set for all of us to use the same datapoints for scoring.\n  - Download dataset from: http://tranquant.com/search?search=HackOn(Data)%20Exercise%2006\n  - First dataset: OnlineNewsTrainingAndValidation.csv\n  - Scoring dataset: OnlineNewsTesting.csv\n  - Note that the data set is from http://archive.ics.uci.edu/ml/datasets/Online+News+Popularity where you cand find information about each field. \n    <pre>K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. Proceedings of the 17th EPIA 2015 - Portuguese Conference on Artificial Intelligence, September, Coimbra, Portugal.</pre>\n    \n- The goal of the regression task is to predict the number of shares an article will generate (last column in the dataset). The features are already generated for you.\n- Use the spark ml implementation for the linear regression:\n\n  example: https://spark.apache.org/docs/1.6.2/ml-classification-regression.html#linear-regression\n  \n  documentation: https://spark.apache.org/docs/1.6.1/api/python/pyspark.ml.html?highlight=linearregression#pyspark.ml.regression.LinearRegression\n  \n  to tune the model parameters, you can use the ParamGridBuilder() of spark.ml.tuning, see more:\n   - https://spark.apache.org/docs/latest/ml-tuning.html\n  \n- To analyze the data, follow a similar strategy as with the lab\n  - Start by doing some data exploration \n  - Would you use all the features? which ones would you remove?\n  - What parameter would you tune?\n  - What metrics would you include?\n  - How can you improve the model?"],"metadata":{}}],"metadata":{"name":"Exercise 6 - Linear Regression","notebookId":1784028329753107},"nbformat":4,"nbformat_minor":0}
